**Gradient Descent Algorithms and Neural Network Implementation**

Overview

- This project explores the fundamental concept of gradient descent, a core optimization algorithm used in machine learning and neural network training. I implemented three variants of gradient descent - batch, stochastic, and mini-batch - from scratch in Python, without relying on high-level libraries like TensorFlow or PyTorch.

- In addition, I built a basic neural network and used the custom gradient descent implementations to train the model and make predictions on a dataset.

The goals of this project were to:

- Develop a deep understanding of how gradient descent works under the hood.
- Gain hands-on experience implementing different gradient descent algorithms.
- Apply gradient descent to train a neural network and generate predictions.
- Explore the strengths, weaknesses, and tradeoffs of the various gradient descent approaches.
